import elasticapm
from asgiref.sync import sync_to_async
from datetime import timedelta
from django.conf import settings
from django.utils import timezone
from django.db.models import Q, Prefetch, Exists, OuterRef, Subquery, Max

from reportcreator_api.pentests.models import UserNotebookPage, UploadedImage, UploadedProjectFile, \
    UploadedUserNotebookImage, UploadedUserNotebookFile, PentestProject, ArchivedProject, \
    FindingTemplate, UploadedTemplateImage
from reportcreator_api.users.models import PentestUser
from reportcreator_api.utils import license


def is_referenced_in_project(project, f):
    # Project data (sections)
    if f.name in str(project.data_all):
        return True
    
    # Findings
    for finding in project.findings.all():
        if f.name in str(finding.data_all):
            return True
    
    # Notes
    for note in project.notes.all():
        if f.name in note.text or f.name in note.title:
            return True
    return False


def is_referenced_in_user_notebook(user, f):
    for note in user.notes.all():
        if f.name in note.text or f.name in note.title:
            return True
    return False


def is_referenced_in_template(template, f):
    for translation in template.translations.all():
        # custom_fields is used instead of data_all, 
        # because accessing data_all would require field_definition which would be fetched by a syncronous DB query
        # Syncronous DB queries cannot be called from the current async function
        if f.name in str(translation.custom_fields):
            return True
    return False


@elasticapm.async_capture_span()
async def cleanup_project_files(task_info):
    # Only cleanup older files, to prevent race conditions: upload -> cleanup -> save text with reference -> referenced file already deleted
    older_than = timezone.now() - timedelta(days=2)
    projects = PentestProject.objects \
        .filter(created__lt=older_than) \
        .select_related('project_type') \
        .prefetch_related(
            'findings', 
            'notes', 
            Prefetch('images', UploadedImage.objects.filter(updated__lt=older_than), to_attr='images_cleanup'),
            Prefetch('files', UploadedProjectFile.objects.filter(updated__lt=older_than), to_attr='files_cleanup'),
        )
    # Only check projects that changed since the last cleanup
    if last_run := task_info['model'].last_success:
        last_run = min(last_run, older_than - timedelta(days=1))
        projects = projects.filter(id__in=projects.filter(
            Q(updated__gt=last_run) | 
            Q(findings__updated__gt=last_run) | 
            Q(sections__updated__gt=last_run) | 
            Q(notes__updated__gt=last_run)
        ).values_list('id'))
    
    # Check if files are referenced
    # Requires checking in python because of DB encryption
    cleanup_images = []
    cleanup_files = []
    async for p in projects:
        for f in p.images_cleanup:
            if not is_referenced_in_project(p, f):
                cleanup_images.append(f)
        for f in p.files_cleanup:
            if not is_referenced_in_project(p, f):
                cleanup_files.append(f)

    if cleanup_images:
        await UploadedImage.objects \
            .filter(pk__in=map(lambda f: f.pk, cleanup_images)) \
            .adelete()
    if cleanup_files:
        await UploadedProjectFile.objects \
            .filter(pk__in=map(lambda f: f.pk, cleanup_files)) \
            .adelete()


@elasticapm.async_capture_span()
async def cleanup_usernotebook_files(task_info):
    older_than = timezone.now() - timedelta(days=2)

    user_notes = UserNotebookPage.objects \
        .filter(user=OuterRef('pk'))
    if last_run := task_info['model'].last_success:
        last_run = min(last_run, older_than - timedelta(days=1))
        user_notes = user_notes.filter(updated__gt=last_run)
    
    images_cleanup = UploadedUserNotebookImage.objects.filter(updated__lt=older_than)
    files_cleanup = UploadedUserNotebookFile.objects.filter(updated__lt=older_than)

    users = PentestUser.objects \
        .filter(created__lt=older_than) \
        .annotate(has_notes=Exists(user_notes)) \
        .annotate(has_files=Q(Exists(images_cleanup.filter(linked_object=OuterRef('pk')))) | Q(Exists(files_cleanup.filter(linked_object=OuterRef('pk'))))) \
        .filter(has_notes=True, has_files=True) \
        .prefetch_related(
            'notes',
            Prefetch('images', images_cleanup, to_attr='images_cleanup'),
            Prefetch('files', files_cleanup, to_attr='files_cleanup'),
        )
    
    cleanup_images = []
    cleanup_files = []
    async for u in users:
        for f in u.images_cleanup:
            if not is_referenced_in_user_notebook(u, f):
                cleanup_images.append(f)
        for f in u.files_cleanup:
            if not is_referenced_in_user_notebook(u, f):
                cleanup_files.append(f)

    if cleanup_images:
        await UploadedUserNotebookImage.objects \
            .filter(pk__in=map(lambda f: f.pk, cleanup_images)) \
            .adelete()
    if cleanup_files:
        await UploadedUserNotebookFile.objects \
            .filter(pk__in=map(lambda f: f.pk, cleanup_files)) \
            .adelete()


@elasticapm.async_capture_span()
async def cleanup_template_files(task_info):
    older_than = timezone.now() - timedelta(days=2)

    images_cleanup = UploadedTemplateImage.objects.filter(updated__lt=older_than)
    templates = FindingTemplate.objects \
        .annotate(has_files=Exists(images_cleanup.filter(linked_object=OuterRef('pk')))) \
        .filter(has_files=True) \
        .prefetch_related(
            'translations',
            Prefetch('images', images_cleanup, to_attr='images_cleanup')
        )
    
    # Only check templates that changed since last cleanup
    if last_run := task_info['model'].last_success:
        last_run = min(last_run, older_than - timedelta(days=1))
        templates = templates.filter(id__in=templates.filter(
            Q(updated__gt=last_run) | 
            Q(translations__updated__gt=last_run)
        ).values_list('id'))
    
    cleanup_images = []
    async for t in templates:
        for f in t.images_cleanup:
            if not is_referenced_in_template(t, f):
                cleanup_images.append(f)

    if cleanup_images:
        await UploadedTemplateImage.objects \
            .filter(pk__in=map(lambda f: f.pk, cleanup_images)) \
            .adelete()


async def cleanup_unreferenced_images_and_files(task_info):
    await cleanup_project_files(task_info)
    await cleanup_usernotebook_files(task_info)
    await cleanup_template_files(task_info)


def reset_stale_archive_restores(task_info):
    """
    Deletes decrypted shamir keys from the database, when archive restore is stale (last decryption more than 3 days ago),
    i.e. some users decrypted their key parts, but some are still missing.
    Prevent decrypted shamir keys being stored in the DB forever.
    """
    from reportcreator_api.pentests.models import ArchivedProjectKeyPart

    ArchivedProjectKeyPart.objects \
        .filter(decrypted_at__isnull=False) \
        .annotate(last_decrypted=Subquery(
            ArchivedProjectKeyPart.objects
            .filter(archived_project=OuterRef('archived_project'))
            .values('archived_project')
            .annotate(last_decrypted=Max('decrypted_at'))
            .values_list('last_decrypted')
        )) \
        .filter(last_decrypted__lt=timezone.now() - settings.AUTOMATICALLY_RESET_STALE_ARCHIVE_RESTORES_AFTER) \
        .update(decrypted_at=None, key_part=None)


async def automatically_archive_projects(task_info):
    if not settings.AUTOMATICALLY_ARCHIVE_PROJECTS_AFTER or not license.is_professional():
        return
    
    projects_to_archive = PentestProject.objects \
        .filter(readonly=True) \
        .filter(readonly_since__lt=timezone.now() - settings.AUTOMATICALLY_ARCHIVE_PROJECTS_AFTER) \
        .only_archivable()
    
    async for p in projects_to_archive:
        await sync_to_async(ArchivedProject.objects.create_from_project)(p)


async def automatically_delete_archived_projects(task_info):
    if not settings.AUTOMATICALLY_DELETE_ARCHIVED_PROJECTS_AFTER or not license.is_professional():
        return
    
    await ArchivedProject.objects \
        .filter(created__lt=timezone.now() - settings.AUTOMATICALLY_DELETE_ARCHIVED_PROJECTS_AFTER) \
        .adelete()